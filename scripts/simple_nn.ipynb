{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5972a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e924ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CSI(CSI_RAW): # Function to extract the CSI information from CSV file rows\n",
    "    string = CSI_RAW\n",
    "    res = string.split(' ')\n",
    "    numbers = []\n",
    "    for token in res:\n",
    "        if '[' in token or ']' in token:\n",
    "            if '[' in token:\n",
    "                temp = token.split('[')\n",
    "                if len(temp[1]) > 0:\n",
    "                    number = int(temp[1])\n",
    "                    numbers.append(number)\n",
    "            if ']' in token:\n",
    "                temp = token.split(']')\n",
    "                if len(temp[1]) > 0:\n",
    "                    number = int(temp[1])\n",
    "                    numbers.append(number)\n",
    "        else:\n",
    "            number = int(token)\n",
    "            numbers.append(number)\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a099e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A seperate CSV file for each location\n",
    "CSV_FILE_1 = '../data/lab_entrance.csv'\n",
    "CSV_FILE_2 = '../data/lab_infront.csv'\n",
    "CSV_FILE_3 = '../data/lab_2ndrow.csv'\n",
    "\n",
    "n_classes = 3\n",
    "CSI_dim = 128\n",
    "\n",
    "target_mac = \"3A:FB:99:B4:E0:FC\" # replace with your target device mac\n",
    "\n",
    "\n",
    "df_1 = pd.read_csv(CSV_FILE_1)\n",
    "df_2 = pd.read_csv(CSV_FILE_2)\n",
    "df_3 = pd.read_csv(CSV_FILE_3)\n",
    "\n",
    "df_1 = df_1.drop(columns='CSI_DATA')\n",
    "df_1 = df_1.rename(columns={'seq_ctrl': 'CSI'})\n",
    "\n",
    "df_2 = df_2.drop(columns='CSI_DATA')\n",
    "df_2 = df_2.rename(columns={'seq_ctrl': 'CSI'})\n",
    "\n",
    "df_3 = df_3.drop(columns='CSI_DATA')\n",
    "df_3 = df_3.rename(columns={'seq_ctrl': 'CSI'})\n",
    "\n",
    "\n",
    "# Take relevant data\n",
    "df_1 = df_1[['mac', 'rssi', 'CSI']] # Relevant columns\n",
    "df_1 = df_1[df_1['mac'] == target_mac] # Take only target mac address into account\n",
    "\n",
    "df_2 = df_2[['mac', 'rssi', 'CSI']] # Relevant columns\n",
    "df_2 = df_2[df_2['mac'] == target_mac] # Take only target mac address into account\n",
    "\n",
    "df_3 = df_3[['mac', 'rssi', 'CSI']] # Relevant columns\n",
    "df_3 = df_3[df_3['mac'] == target_mac] # Take only target mac address into account\n",
    "\n",
    "data_1 = df_1[['CSI']].to_numpy()\n",
    "data_2 = df_2[['CSI']].to_numpy()\n",
    "data_3 = df_3[['CSI']].to_numpy()\n",
    "\n",
    "n_data_1 = len(data_1)\n",
    "n_data_2 = len(data_2)\n",
    "n_data_3 = len(data_3)\n",
    "n_data = n_data_1 + n_data_2 + n_data_3\n",
    "\n",
    "X = np.zeros((n_data, CSI_dim)) # Features\n",
    "Y = np.zeros((n_data, n_classes)) # Labels\n",
    "\n",
    "for i, value in enumerate(data_1):\n",
    "    string = value[0]\n",
    "    X[i,:] = np.array(get_CSI(string))\n",
    "    Y[i,0] = 1\n",
    "\n",
    "for i, value in enumerate(data_2):\n",
    "    string = value[0]\n",
    "    X[i+n_data_1,:] = np.array(get_CSI(string))\n",
    "    Y[i+n_data_1,1] = 1\n",
    "\n",
    "for i, value in enumerate(data_3):\n",
    "    string = value[0]\n",
    "    X[i+n_data_1+n_data_2,:] = np.array(get_CSI(string))\n",
    "    Y[i+n_data_1+n_data_2,2] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dcf3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle inputs\n",
    "indsh = (np.arange(n_data))\n",
    "np.random.shuffle(indsh)\n",
    "\n",
    "X = X[indsh, :]\n",
    "Y = Y[indsh]\n",
    "\n",
    "X_train = X[:int(0.7 * n_data),:]\n",
    "y_train = Y[:int(0.7 * n_data),:]\n",
    "\n",
    "X_val = X[int(0.7 * n_data):int(0.8 * n_data),:]\n",
    "y_val = Y[int(0.7 * n_data):int(0.8 * n_data),:]\n",
    "\n",
    "X_test = X[int(0.8 * n_data):,:]\n",
    "y_test = Y[int(0.8 * n_data):,:]\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, feature, label):\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.feature[idx, :]\n",
    "        y = self.label[idx, :]\n",
    "        return x, y\n",
    "\n",
    "# Create Dataset objects\n",
    "train_data = CustomDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "val_data = CustomDataset(torch.Tensor(X_val), torch.Tensor(y_val))\n",
    "test_data = CustomDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "\n",
    "# Create DataLoader for training, validation, and test sets\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93658cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_layers = nn.Sequential(\n",
    "    nn.Linear(CSI_dim, 256, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Linear(256, 64, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 32, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.Linear(32, 16, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16),\n",
    "    nn.Linear(16, 4, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(4),\n",
    "    nn.Linear(4, n_classes, bias=True) # Last dim is the amount classes / locations\n",
    ")\n",
    "\n",
    "class My_NN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(My_NN, self).__init__()\n",
    "            self.block = shared_layers\n",
    "\n",
    "        def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.kaiming_uniform_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.BatchNorm1d):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "        def forward(self, feature):  # input feature matrix size: CSI_dim = 128\n",
    "            output1 = self.block(feature)\n",
    "\n",
    "            return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298b0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:32<00:03,  2.67it/s]"
     ]
    }
   ],
   "source": [
    "# Initialize the model and other hyperparameters\n",
    "my_nn = My_NN()\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(my_nn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "my_nn.train()\n",
    "num_epochs = 100\n",
    "print_parameters_every = 20  # Print parameters every n epochs\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for feature_batch, labels_batch in train_loader:  \n",
    "        output = my_nn(feature_batch)  \n",
    "        loss = criterion(output, labels_batch)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())  # Store training loss\n",
    "    # validation loop\n",
    "    with torch.no_grad():\n",
    "        for feature_batch, labels_batch in val_loader:\n",
    "            output = my_nn(feature_batch)\n",
    "            val_loss = criterion(output, labels_batch)\n",
    "            # scheduler.step(val_loss)  # you may add the scheduler to avoid over-fitting by reducing learning rate\n",
    "\n",
    "        validation_losses.append(val_loss.item())\n",
    "# Plot the training and validation loss\n",
    "plt.figure(1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b973e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.8984375\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "my_nn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for feature_batch, labels_batch in test_loader:\n",
    "        out = my_nn(feature_batch)\n",
    "        _,pred = torch.max(out,1)\n",
    "        _, real = torch.max(labels_batch,1)\n",
    "        total += len(real)\n",
    "        correct += (pred == real).sum().item()\n",
    "\n",
    "print(\"Accuracy:\", correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
